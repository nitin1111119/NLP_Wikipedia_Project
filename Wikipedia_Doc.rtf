{\rtf1\ansi\ansicpg1252\cocoartf2709
\cocoatextscaling0\cocoaplatform0{\fonttbl\f0\fswiss\fcharset0 Helvetica;\f1\fswiss\fcharset0 Helvetica-Bold;}
{\colortbl;\red255\green255\blue255;\red31\green35\blue38;\red255\green255\blue255;\red60\green69\blue75;
}
{\*\expandedcolortbl;;\cssrgb\c16078\c18431\c19608;\cssrgb\c100000\c100000\c100000;\cssrgb\c30196\c34118\c36471;
}
{\*\listtable{\list\listtemplateid1\listhybrid{\listlevel\levelnfc0\levelnfcn0\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{decimal\}}{\leveltext\leveltemplateid1\'01\'00;}{\levelnumbers\'01;}\fi-360\li720\lin720 }{\listlevel\levelnfc0\levelnfcn0\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{decimal\}}{\leveltext\leveltemplateid2\'01\'01;}{\levelnumbers\'01;}\fi-360\li1440\lin1440 }{\listname ;}\listid1}}
{\*\listoverridetable{\listoverride\listid1\listoverridecount0\ls1}}
\paperw11900\paperh16840\margl1440\margr1440\vieww11520\viewh8400\viewkind0
\deftab720
\pard\pardeftab720\sa240\partightenfactor0

\f0\fs28 \cf2 \cb3 \expnd0\expndtw0\kerning0
\outl0\strokewidth0 \strokec2 DESCRIPTION\
\pard\pardeftab720\sa200\partightenfactor0
\cf4 \strokec4 Using NLP and machine learning, make a model to identify toxic comments from the Talk edit pages on Wikipedia. Help identify the words that make a comment toxic.\cb1 \
\pard\pardeftab720\sa200\partightenfactor0

\f1\b \cf4 \cb3 Problem Statement:\'a0\'a0
\f0\b0 \cb1 \
\pard\pardeftab720\sa200\partightenfactor0
\cf4 \cb3 Wikipedia is the world\'92s largest and most popular reference work on the internet with about 500 million unique visitors per month. It also has millions of contributors who can make edits to pages. The Talk edit pages, the key community interaction forum where the contributing community interacts or discusses or debates about the changes pertaining to a particular topic.\'a0\cb1 \
\cb3 Wikipedia continuously strives to help online discussion become more productive and respectful. You are a data scientist at Wikipedia who will help Wikipedia to build a predictive model that identifies toxic comments in the discussion and marks them for cleanup by using NLP and machine learning. Post that, help identify the top terms from the toxic comments.\'a0\cb1 \
\pard\pardeftab720\sa200\partightenfactor0

\f1\b \cf4 \cb3 Domain
\f0\b0 : Internet\cb1 \

\f1\b \cb3 Analysis to be done
\f0\b0 : Build a text classification model using NLP and machine learning that detects toxic comments.\cb1 \

\f1\b \cb3 Content:\'a0
\f0\b0 \cb1 \
\pard\pardeftab720\sa200\partightenfactor0
\cf4 \cb3 id: identifier number of the comment\cb1 \
\cb3 comment_text: the text in the comment\cb1 \
\cb3 toxic: 0 (non-toxic) /1 (toxic)\cb1 \
\pard\pardeftab720\sa200\partightenfactor0

\f1\b \cf4 \cb3 Steps to perform:
\f0\b0 \cb1 \
\pard\pardeftab720\sa200\partightenfactor0
\cf4 \cb3 Cleanup the text data, using TF-IDF convert to vector space representation, use Support Vector Machines to detect toxic comments. Finally, get the list of top 15 toxic terms from the comments identified by the model.\cb1 \
\pard\pardeftab720\sa200\partightenfactor0

\f1\b \cf4 \cb3 Tasks:
\f0\b0 \'a0\cb1 \
\pard\tx220\tx720\pardeftab720\li720\fi-720\sa200\partightenfactor0
\ls1\ilvl0\cf4 \cb3 \kerning1\expnd0\expndtw0 \outl0\strokewidth0 {\listtext	1	}\expnd0\expndtw0\kerning0
\outl0\strokewidth0 \strokec4 Load the data using read_csv function from pandas package\cb1 \
\ls1\ilvl0\cb3 \kerning1\expnd0\expndtw0 \outl0\strokewidth0 {\listtext	2	}\expnd0\expndtw0\kerning0
\outl0\strokewidth0 \strokec4 Get the comments into a list, for easy text cleanup and manipulation\cb1 \
\ls1\ilvl0\cb3 \kerning1\expnd0\expndtw0 \outl0\strokewidth0 {\listtext	3	}\expnd0\expndtw0\kerning0
\outl0\strokewidth0 \strokec4 Cleanup:\'a0\cb1 \
\pard\tx940\tx1440\pardeftab720\li1440\fi-1440\sa200\partightenfactor0
\ls1\ilvl1\cf4 \cb3 \kerning1\expnd0\expndtw0 \outl0\strokewidth0 {\listtext	1	}\expnd0\expndtw0\kerning0
\outl0\strokewidth0 \strokec4 Using regular expressions, remove IP addresses\cb1 \
\ls1\ilvl1\cb3 \kerning1\expnd0\expndtw0 \outl0\strokewidth0 {\listtext	2	}\expnd0\expndtw0\kerning0
\outl0\strokewidth0 \strokec4 Using regular expressions, remove URLs\cb1 \
\ls1\ilvl1\cb3 \kerning1\expnd0\expndtw0 \outl0\strokewidth0 {\listtext	3	}\expnd0\expndtw0\kerning0
\outl0\strokewidth0 \strokec4 Normalize the casing\cb1 \
\ls1\ilvl1\cb3 \kerning1\expnd0\expndtw0 \outl0\strokewidth0 {\listtext	4	}\expnd0\expndtw0\kerning0
\outl0\strokewidth0 \strokec4 Tokenize using word_tokenize from NLTK\cb1 \
\ls1\ilvl1\cb3 \kerning1\expnd0\expndtw0 \outl0\strokewidth0 {\listtext	5	}\expnd0\expndtw0\kerning0
\outl0\strokewidth0 \strokec4 Remove stop words\cb1 \
\ls1\ilvl1\cb3 \kerning1\expnd0\expndtw0 \outl0\strokewidth0 {\listtext	6	}\expnd0\expndtw0\kerning0
\outl0\strokewidth0 \strokec4 Remove punctuation\cb1 \
\ls1\ilvl1\cb3 \kerning1\expnd0\expndtw0 \outl0\strokewidth0 {\listtext	7	}\expnd0\expndtw0\kerning0
\outl0\strokewidth0 \strokec4 Define a function to perform all these steps, you\'92ll use this later on the actual test set\cb1 \
\pard\tx220\tx720\pardeftab720\li720\fi-720\sa200\partightenfactor0
\ls1\ilvl0\cf4 \cb3 \kerning1\expnd0\expndtw0 \outl0\strokewidth0 {\listtext	4	}\expnd0\expndtw0\kerning0
\outl0\strokewidth0 \strokec4 Using a counter, find the top terms in the data.\'a0\cb1 \
\pard\tx940\tx1440\pardeftab720\li1440\fi-1440\sa200\partightenfactor0
\ls1\ilvl1\cf4 \cb3 \kerning1\expnd0\expndtw0 \outl0\strokewidth0 {\listtext	1	}\expnd0\expndtw0\kerning0
\outl0\strokewidth0 \strokec4 Can any of these be considered contextual stop words?\'a0\cb1 \
\ls1\ilvl1\cb3 \kerning1\expnd0\expndtw0 \outl0\strokewidth0 {\listtext	2	}\expnd0\expndtw0\kerning0
\outl0\strokewidth0 \strokec4 Words like \'93Wikipedia\'94, \'93page\'94, \'93edit\'94 are examples of contextual stop words\cb1 \
\ls1\ilvl1\cb3 \kerning1\expnd0\expndtw0 \outl0\strokewidth0 {\listtext	3	}\expnd0\expndtw0\kerning0
\outl0\strokewidth0 \strokec4 If yes, drop these from the data\cb1 \
\pard\tx220\tx720\pardeftab720\li720\fi-720\sa200\partightenfactor0
\ls1\ilvl0\cf4 \cb3 \kerning1\expnd0\expndtw0 \outl0\strokewidth0 {\listtext	5	}\expnd0\expndtw0\kerning0
\outl0\strokewidth0 \strokec4 Separate into train and test sets\cb1 \
\pard\tx940\tx1440\pardeftab720\li1440\fi-1440\sa200\partightenfactor0
\ls1\ilvl1\cf4 \cb3 \kerning1\expnd0\expndtw0 \outl0\strokewidth0 {\listtext	1	}\expnd0\expndtw0\kerning0
\outl0\strokewidth0 \strokec4 Use train-test method to divide your data into 2 sets: train and test\cb1 \
\ls1\ilvl1\cb3 \kerning1\expnd0\expndtw0 \outl0\strokewidth0 {\listtext	2	}\expnd0\expndtw0\kerning0
\outl0\strokewidth0 \strokec4 Use a 70-30 split\cb1 \
\pard\tx220\tx720\pardeftab720\li720\fi-720\sa200\partightenfactor0
\ls1\ilvl0\cf4 \cb3 \kerning1\expnd0\expndtw0 \outl0\strokewidth0 {\listtext	6	}\expnd0\expndtw0\kerning0
\outl0\strokewidth0 \strokec4 Use TF-IDF values for the terms as feature to get into a vector space model\cb1 \
\pard\tx940\tx1440\pardeftab720\li1440\fi-1440\sa200\partightenfactor0
\ls1\ilvl1\cf4 \cb3 \kerning1\expnd0\expndtw0 \outl0\strokewidth0 {\listtext	1	}\expnd0\expndtw0\kerning0
\outl0\strokewidth0 \strokec4 Import TF-IDF vectorizer from sklearn\cb1 \
\ls1\ilvl1\cb3 \kerning1\expnd0\expndtw0 \outl0\strokewidth0 {\listtext	2	}\expnd0\expndtw0\kerning0
\outl0\strokewidth0 \strokec4 Instantiate with a maximum of 4000 terms in your vocabulary\cb1 \
\ls1\ilvl1\cb3 \kerning1\expnd0\expndtw0 \outl0\strokewidth0 {\listtext	3	}\expnd0\expndtw0\kerning0
\outl0\strokewidth0 \strokec4 Fit and apply on the train set\cb1 \
\ls1\ilvl1\cb3 \kerning1\expnd0\expndtw0 \outl0\strokewidth0 {\listtext	4	}\expnd0\expndtw0\kerning0
\outl0\strokewidth0 \strokec4 Apply on the test set\cb1 \
\pard\tx220\tx720\pardeftab720\li720\fi-720\sa200\partightenfactor0
\ls1\ilvl0\cf4 \cb3 \kerning1\expnd0\expndtw0 \outl0\strokewidth0 {\listtext	7	}\expnd0\expndtw0\kerning0
\outl0\strokewidth0 \strokec4 Model building: Support Vector Machine\cb1 \
\pard\tx940\tx1440\pardeftab720\li1440\fi-1440\sa200\partightenfactor0
\ls1\ilvl1\cf4 \cb3 \kerning1\expnd0\expndtw0 \outl0\strokewidth0 {\listtext	1	}\expnd0\expndtw0\kerning0
\outl0\strokewidth0 \strokec4 Instantiate SVC from sklearn with a linear kernel\cb1 \
\ls1\ilvl1\cb3 \kerning1\expnd0\expndtw0 \outl0\strokewidth0 {\listtext	2	}\expnd0\expndtw0\kerning0
\outl0\strokewidth0 \strokec4 Fit on the train data\cb1 \
\ls1\ilvl1\cb3 \kerning1\expnd0\expndtw0 \outl0\strokewidth0 {\listtext	3	}\expnd0\expndtw0\kerning0
\outl0\strokewidth0 \strokec4 Make predictions for the train and the test set\cb1 \
\pard\tx220\tx720\pardeftab720\li720\fi-720\sa200\partightenfactor0
\ls1\ilvl0\cf4 \cb3 \kerning1\expnd0\expndtw0 \outl0\strokewidth0 {\listtext	8	}\expnd0\expndtw0\kerning0
\outl0\strokewidth0 \strokec4 Model evaluation: Accuracy, recall, and f1_score\cb1 \
\pard\tx940\tx1440\pardeftab720\li1440\fi-1440\sa200\partightenfactor0
\ls1\ilvl1\cf4 \cb3 \kerning1\expnd0\expndtw0 \outl0\strokewidth0 {\listtext	1	}\expnd0\expndtw0\kerning0
\outl0\strokewidth0 \strokec4 Report the accuracy on the train set\cb1 \
\ls1\ilvl1\cb3 \kerning1\expnd0\expndtw0 \outl0\strokewidth0 {\listtext	2	}\expnd0\expndtw0\kerning0
\outl0\strokewidth0 \strokec4 Report the recall on the train set:decent, high, low?\cb1 \
\ls1\ilvl1\cb3 \kerning1\expnd0\expndtw0 \outl0\strokewidth0 {\listtext	3	}\expnd0\expndtw0\kerning0
\outl0\strokewidth0 \strokec4 Get the f1_score on the train set\cb1 \
\pard\tx220\tx720\pardeftab720\li720\fi-720\sa200\partightenfactor0
\ls1\ilvl0\cf4 \cb3 \kerning1\expnd0\expndtw0 \outl0\strokewidth0 {\listtext	9	}\expnd0\expndtw0\kerning0
\outl0\strokewidth0 \strokec4 Looks like you need to adjust\'a0 the class imbalance, as the model seems to focus on the 0s\cb1 \
\pard\tx940\tx1440\pardeftab720\li1440\fi-1440\sa200\partightenfactor0
\ls1\ilvl1\cf4 \cb3 \kerning1\expnd0\expndtw0 \outl0\strokewidth0 {\listtext	1	}\expnd0\expndtw0\kerning0
\outl0\strokewidth0 \strokec4 Adjust the appropriate parameter in the SVC module\cb1 \
\pard\tx220\tx720\pardeftab720\li720\fi-720\sa200\partightenfactor0
\ls1\ilvl0\cf4 \cb3 \kerning1\expnd0\expndtw0 \outl0\strokewidth0 {\listtext	10	}\expnd0\expndtw0\kerning0
\outl0\strokewidth0 \strokec4 Train again with the adjustment and evaluate\cb1 \
\pard\tx940\tx1440\pardeftab720\li1440\fi-1440\sa200\partightenfactor0
\ls1\ilvl1\cf4 \cb3 \kerning1\expnd0\expndtw0 \outl0\strokewidth0 {\listtext	1	}\expnd0\expndtw0\kerning0
\outl0\strokewidth0 \strokec4 Train the model on the train set\cb1 \
\ls1\ilvl1\cb3 \kerning1\expnd0\expndtw0 \outl0\strokewidth0 {\listtext	2	}\expnd0\expndtw0\kerning0
\outl0\strokewidth0 \strokec4 Evaluate the predictions on the validation set: accuracy, recall, f1_score\cb1 \
\pard\tx220\tx720\pardeftab720\li720\fi-720\sa200\partightenfactor0
\ls1\ilvl0\cf4 \cb3 \kerning1\expnd0\expndtw0 \outl0\strokewidth0 {\listtext	11	}\expnd0\expndtw0\kerning0
\outl0\strokewidth0 \strokec4 Hyperparameter tuning\cb1 \
\pard\tx940\tx1440\pardeftab720\li1440\fi-1440\sa200\partightenfactor0
\ls1\ilvl1\cf4 \cb3 \kerning1\expnd0\expndtw0 \outl0\strokewidth0 {\listtext	1	}\expnd0\expndtw0\kerning0
\outl0\strokewidth0 \strokec4 Import GridSearch and StratifiedKFold (because of class imbalance)\cb1 \
\ls1\ilvl1\cb3 \kerning1\expnd0\expndtw0 \outl0\strokewidth0 {\listtext	2	}\expnd0\expndtw0\kerning0
\outl0\strokewidth0 \strokec4 Provide the parameter grid to choose for \'91C\'92\cb1 \
\ls1\ilvl1\cb3 \kerning1\expnd0\expndtw0 \outl0\strokewidth0 {\listtext	3	}\expnd0\expndtw0\kerning0
\outl0\strokewidth0 \strokec4 Use a balanced class weight while instantiating the Support Vector Classifier\cb1 \
\pard\tx220\tx720\pardeftab720\li720\fi-720\sa200\partightenfactor0
\ls1\ilvl0\cf4 \cb3 \kerning1\expnd0\expndtw0 \outl0\strokewidth0 {\listtext	12	}\expnd0\expndtw0\kerning0
\outl0\strokewidth0 \strokec4 Find the parameters with the best recall in cross validation\cb1 \
\pard\tx940\tx1440\pardeftab720\li1440\fi-1440\sa200\partightenfactor0
\ls1\ilvl1\cf4 \cb3 \kerning1\expnd0\expndtw0 \outl0\strokewidth0 {\listtext	1	}\expnd0\expndtw0\kerning0
\outl0\strokewidth0 \strokec4 Choose \'91recall\'92 as the metric for scoring\cb1 \
\ls1\ilvl1\cb3 \kerning1\expnd0\expndtw0 \outl0\strokewidth0 {\listtext	2	}\expnd0\expndtw0\kerning0
\outl0\strokewidth0 \strokec4 Choose stratified 5 fold cross validation scheme\cb1 \
\ls1\ilvl1\cb3 \kerning1\expnd0\expndtw0 \outl0\strokewidth0 {\listtext	3	}\expnd0\expndtw0\kerning0
\outl0\strokewidth0 \strokec4 Fit on the train set\cb1 \
\pard\tx220\tx720\pardeftab720\li720\fi-720\sa200\partightenfactor0
\ls1\ilvl0\cf4 \cb3 \kerning1\expnd0\expndtw0 \outl0\strokewidth0 {\listtext	13	}\expnd0\expndtw0\kerning0
\outl0\strokewidth0 \strokec4 What are the best parameters?\cb1 \
\ls1\ilvl0\cb3 \kerning1\expnd0\expndtw0 \outl0\strokewidth0 {\listtext	14	}\expnd0\expndtw0\kerning0
\outl0\strokewidth0 \strokec4 Predict and evaluate using the best estimator\cb1 \
\pard\tx940\tx1440\pardeftab720\li1440\fi-1440\sa200\partightenfactor0
\ls1\ilvl1\cf4 \cb3 \kerning1\expnd0\expndtw0 \outl0\strokewidth0 {\listtext	1	}\expnd0\expndtw0\kerning0
\outl0\strokewidth0 \strokec4 Use best estimator from the grid search to make predictions on the test set\cb1 \
\ls1\ilvl1\cb3 \kerning1\expnd0\expndtw0 \outl0\strokewidth0 {\listtext	2	}\expnd0\expndtw0\kerning0
\outl0\strokewidth0 \strokec4 What is the recall on the test set for the toxic comments?\cb1 \
\ls1\ilvl1\cb3 \kerning1\expnd0\expndtw0 \outl0\strokewidth0 {\listtext	3	}\expnd0\expndtw0\kerning0
\outl0\strokewidth0 \strokec4 What is the f1_score?\cb1 \
\pard\tx220\tx720\pardeftab720\li720\fi-720\sa200\partightenfactor0
\ls1\ilvl0\cf4 \cb3 \kerning1\expnd0\expndtw0 \outl0\strokewidth0 {\listtext	15	}\expnd0\expndtw0\kerning0
\outl0\strokewidth0 \strokec4 What are the most prominent terms in the toxic comments?\cb1 \
\pard\tx940\tx1440\pardeftab720\li1440\fi-1440\sa200\partightenfactor0
\ls1\ilvl1\cf4 \cb3 \kerning1\expnd0\expndtw0 \outl0\strokewidth0 {\listtext	1	}\expnd0\expndtw0\kerning0
\outl0\strokewidth0 \strokec4 Separate the comments from the test set that the model identified as toxic\cb1 \
\ls1\ilvl1\cb3 \kerning1\expnd0\expndtw0 \outl0\strokewidth0 {\listtext	2	}\expnd0\expndtw0\kerning0
\outl0\strokewidth0 \strokec4 Make one large list of the terms\cb1 \
\ls1\ilvl1\cb3 \kerning1\expnd0\expndtw0 \outl0\strokewidth0 {\listtext	3	}\expnd0\expndtw0\kerning0
\outl0\strokewidth0 \strokec4 Get the top 15 terms\cb1 \
}